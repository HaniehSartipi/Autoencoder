

!pip install tensorflow==2.12.0



import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.datasets import fashion_mnist
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, BatchNormalization
from tensorflow.keras.regularizers import l2
from tensorflow.keras.optimizers import Adam



(x_train, _), (x_test, _) = fashion_mnist.load_data()



x_train = x_train.astype('float32') / 255.0
x_test = x_test.astype('float32') / 255.0



x_train = x_train.reshape(-1, 28 * 28)
x_test = x_test.reshape(-1, 28 * 28)



input_img = Input(shape=(784,))
encoded = Dense(450, activation='tanh', activity_regularizer=l2(1e-5))(input_img)
encoded = BatchNormalization()(encoded)
encoded = Dense(150, activation='tanh', activity_regularizer=l2(1e-5))(encoded)
encoded = BatchNormalization()(encoded)
encoded = Dense(64, activation='tanh', activity_regularizer=l2(1e-5))(encoded)
encoded = BatchNormalization()(encoded)
bottleneck = Dense(32, activation='tanh', activity_regularizer=l2(1e-5))(encoded)
encoded = BatchNormalization()(bottleneck)

decoded = Dense(64, activation='tanh', activity_regularizer=l2(1e-5))(encoded)
decoded = BatchNormalization()(decoded)
decoded = Dense(150, activation='tanh', activity_regularizer=l2(1e-5))(decoded)
decoded = BatchNormalization()(decoded)
decoded = Dense(450, activation='tanh', activity_regularizer=l2(1e-5))(decoded)
decoded = BatchNormalization()(decoded)
decoded = Dense(784, activation='sigmoid')(decoded)

autoencoder = Model(input_img, decoded)



autoencoder.compile(optimizer=Adam(), loss='mean_squared_error', metrics=['accuracy'])

history = autoencoder.fit(x_train, x_train,
                          epochs=50,
                          batch_size=1024,
                          shuffle=True,
                          validation_data=(x_test, x_test))


plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Accuracy Plot')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Loss Plot')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.show()

input_img = Input(shape=(784,))
encoded = Dense(450, activation='relu', activity_regularizer=l2(1e-5))(input_img)
encoded = BatchNormalization()(encoded)
encoded = Dense(150, activation='relu', activity_regularizer=l2(1e-5))(encoded)
encoded = BatchNormalization()(encoded)
encoded = Dense(64, activation='relu', activity_regularizer=l2(1e-5))(encoded)
encoded = BatchNormalization()(encoded)
bottleneck = Dense(32, activation='relu', activity_regularizer=l2(1e-5))(encoded)
encoded = BatchNormalization()(bottleneck)

decoded = Dense(64, activation='relu', activity_regularizer=l2(1e-5))(encoded)
decoded = BatchNormalization()(decoded)
decoded = Dense(150, activation='relu', activity_regularizer=l2(1e-5))(decoded)
decoded = BatchNormalization()(decoded)
decoded = Dense(450, activation='relu', activity_regularizer=l2(1e-5))(decoded)
decoded = BatchNormalization()(decoded)
decoded = Dense(784, activation='sigmoid')(decoded)

autoencoder = Model(input_img, decoded)


autoencoder.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])



plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Accuracy Plot')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Loss Plot')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.show()



history = autoencoder.fit(x_train, x_train,
                          epochs=50,
                          batch_size=128,
                          shuffle=True,
                          validation_data=(x_test, x_test))




plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Accuracy Plot')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Loss Plot')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.show()



input_img = Input(shape=(784,))
encoded = Dense(450, activation='leaky_relu', activity_regularizer=l2(1e-5))(input_img)
encoded = BatchNormalization()(encoded)
encoded = Dense(150, activation='leaky_relu', activity_regularizer=l2(1e-5))(encoded)
encoded = BatchNormalization()(encoded)
encoded = Dense(64, activation='leaky_relu', activity_regularizer=l2(1e-5))(encoded)
encoded = BatchNormalization()(encoded)
bottleneck = Dense(32, activation='leaky_relu', activity_regularizer=l2(1e-5))(encoded)
encoded = BatchNormalization()(bottleneck)

decoded = Dense(64, activation='leaky_relu', activity_regularizer=l2(1e-5))(encoded)
decoded = BatchNormalization()(decoded)
decoded = Dense(150, activation='leaky_relu', activity_regularizer=l2(1e-5))(decoded)
decoded = BatchNormalization()(decoded)
decoded = Dense(450, activation='leaky_relu', activity_regularizer=l2(1e-5))(decoded)
decoded = BatchNormalization()(decoded)
decoded = Dense(784, activation='sigmoid')(decoded)

autoencoder = Model(input_img, decoded)



autoencoder.compile(optimizer=Adam(), loss='mean_squared_error', metrics=['accuracy'])



history = autoencoder.fit(x_train, x_train,
                          epochs=50,
                          batch_size=256,
                          shuffle=True,
                          validation_data=(x_test, x_test))

plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Accuracy Plot')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Loss Plot')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.show()



input_img = Input(shape=(784,))
encoded = Dense(450, activation='leaky_relu', activity_regularizer=l2(1e-5))(input_img)
encoded = BatchNormalization()(encoded)
encoded = Dense(150, activation='leaky_relu', activity_regularizer=l2(1e-5))(encoded)
encoded = BatchNormalization()(encoded)
encoded = Dense(64, activation='leaky_relu', activity_regularizer=l2(1e-5))(encoded)
encoded = BatchNormalization()(encoded)
bottleneck = Dense(32, activation='leaky_relu', activity_regularizer=l2(1e-5))(encoded)
encoded = BatchNormalization()(bottleneck)

decoded = Dense(64, activation='leaky_relu', activity_regularizer=l2(1e-5))(encoded)
decoded = BatchNormalization()(decoded)
decoded = Dense(150, activation='leaky_relu', activity_regularizer=l2(1e-5))(decoded)
decoded = BatchNormalization()(decoded)
decoded = Dense(450, activation='leaky_relu', activity_regularizer=l2(1e-5))(decoded)
decoded = BatchNormalization()(decoded)
decoded = Dense(784, activation='sigmoid')(decoded)

autoencoder = Model(input_img, decoded)

autoencoder.compile(optimizer=Adam(), loss='mean_squared_error', metrics=['accuracy'])

import numpy as np
import matplotlib.pyplot as plt
import random
from tensorflow.keras.layers import Input, Dense, BatchNormalization
from tensorflow.keras.models import Model
from tensorflow.keras.datasets import fashion_mnist
from tensorflow.keras.regularizers import l2
from tensorflow.keras.optimizers import Adam

(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()

x_train = x_train.astype('float32') / 255.0
x_test = x_test.astype('float32') / 255.0

x_train = x_train.reshape(-1, 28 * 28)
x_test = x_test.reshape(-1, 28 * 28)

input_img = Input(shape=(784,))
encoded = Dense(450, activation='leaky_relu', activity_regularizer=l2(1e-5))(input_img)
encoded = BatchNormalization()(encoded)
encoded = Dense(150, activation='leaky_relu', activity_regularizer=l2(1e-5))(encoded)
encoded = BatchNormalization()(encoded)
encoded = Dense(64, activation='leaky_relu', activity_regularizer=l2(1e-5))(encoded)
encoded = BatchNormalization()(encoded)
bottleneck = Dense(32, activation='leaky_relu', activity_regularizer=l2(1e-5))(encoded)
encoded = BatchNormalization()(bottleneck)

decoded = Dense(64, activation='leaky_relu', activity_regularizer=l2(1e-5))(encoded)
decoded = BatchNormalization()(decoded)
decoded = Dense(150, activation='leaky_relu', activity_regularizer=l2(1e-5))(decoded)
decoded = BatchNormalization()(decoded)
decoded = Dense(450, activation='leaky_relu', activity_regularizer=l2(1e-5))(decoded)
decoded = BatchNormalization()(decoded)
decoded = Dense(784, activation='sigmoid')(decoded)

autoencoder = Model(input_img, decoded)

autoencoder.compile(optimizer=Adam(), loss='mean_squared_error', metrics=['accuracy'])

autoencoder.fit(x_train, x_train,
                epochs=50,
                batch_size=256,
                shuffle=True,
                validation_data=(x_test, x_test))
bottleneck_model = Model(inputs=autoencoder.input, outputs=bottleneck)

def plot_images(original, bottleneck, reconstructed, ax, title):
    ax[0].imshow(original.reshape(28, 28), cmap='gray')
    ax[0].set_title('Original')
    ax[0].axis('off')

    bottleneck_image = bottleneck.reshape(4, 8)
    ax[1].imshow(bottleneck_image, cmap='gray')
    ax[1].set_title('Bottleneck')
    ax[1].axis('off')

    ax[2].imshow(reconstructed.reshape(28, 28), cmap='gray')
    ax[2].set_title('Reconstructed')
    ax[2].axis('off')

num_classes = 10
fig, axes = plt.subplots(num_classes, 3, figsize=(12, num_classes * 3))

for class_label in range(num_classes):
    class_indices = np.where(y_test == class_label)[0]
    random_index = random.choice(class_indices)

    original_image = x_test[random_index]
    bottleneck_features = bottleneck_model.predict(np.expand_dims(original_image, axis=0)).squeeze()
    reconstructed_image = autoencoder.predict(np.expand_dims(original_image, axis=0)).squeeze()

    plot_images(original_image, bottleneck_features, reconstructed_image, axes[class_label], title=f"Class {class_label}")

plt.tight_layout()
plt.show()



x_train_flat = x_train.reshape(-1, 28 * 28)
x_test_flat = x_test.reshape(-1, 28 * 28)

bottleneck_model = Model(inputs=autoencoder.input, outputs=bottleneck)

x_train_bottleneck = bottleneck_model.predict(x_train_flat)
x_test_bottleneck = bottleneck_model.predict(x_test_flat)

y_train_one_hot = to_categorical(y_train, num_classes=10)
y_test_one_hot = to_categorical(y_test, num_classes=10)



import matplotlib.pyplot as plt
from tensorflow.keras.callbacks import EarlyStopping

classifier_input = Input(shape=(32,))
x = Dense(64, activation='relu')(classifier_input)
x = BatchNormalization()(x)
x = Dense(32, activation='relu')(x)
x = BatchNormalization()(x)
classifier_output = Dense(10, activation='softmax')(x)

classifier = Model(classifier_input, classifier_output)
classifier.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])

early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

history = classifier.fit(x_train_bottleneck, y_train_one_hot,
                         epochs=50,
                         batch_size=128,
                         validation_split=0.2,
                         callbacks=[early_stopping])

plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Accuracy Plot')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Loss Plot')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.show()

test_loss, test_accuracy = classifier.evaluate(x_test_bottleneck, y_test_one_hot)
print(f'Test Accuracy: {test_accuracy}')

